{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiwoz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unr-N5aqa1Zb"
      },
      "source": [
        "# Task 1: MultiWOZ2.2 Template Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TDg0nXXpMkD"
      },
      "source": [
        "##Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8wAUwiQpDCV",
        "outputId": "e4c79b1e-f146-48c3-c681-02b5831362b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H97WacnyxIBw"
      },
      "source": [
        "#requirements\n",
        "!pip install fastDamerauLevenshtein\n",
        "!pip install g2p_en\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmXsbmcnpYGa",
        "outputId": "bcb43dee-b37f-4ed3-83ac-65c930d559e5"
      },
      "source": [
        "%env HOME=/content/drive/MyDrive/\n",
        "%cd ~/Research/ner/Multiwoz/multiwoz/data/MultiWOZ_2.2/run_env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: HOME=/content/drive/MyDrive/\n",
            "/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/data/MultiWOZ_2.2/run_env\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IhPQKlip0l7",
        "outputId": "7ca08f9b-5dc1-47d2-f303-59ff099aa9e4"
      },
      "source": [
        "!ls -lrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 51\n",
            "-rw------- 1 root root  7376 Aug  6 13:59 extra_methods.py\n",
            "-rw------- 1 root root 25332 Aug  6 13:59 BabyTrie_DSTC10_v4.py\n",
            "-rw------- 1 root root  1120 Aug  7 08:44 simple_tokenize.py\n",
            "drwx------ 2 root root  4096 Aug  7 08:52 __pycache__\n",
            "-rw------- 1 root root 12765 Aug  7 08:52 BabyTrie_MultiWOZ.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9NUjH6B-crs"
      },
      "source": [
        "##Taxi, Train/Bus db modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUC_NuzF-jFR"
      },
      "source": [
        "import json\n",
        "\n",
        "#Taxi\n",
        "db = open('/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/taxi_db.json','r')\n",
        "taxi_dic = json.load(db)\n",
        "db.close()\n",
        "new_db = open('/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/taxi_db_clean.json','w')\n",
        "\n",
        "taxi_final = []\n",
        "colors = taxi_dic['taxi_colors']\n",
        "types = taxi_dic['taxi_types']\n",
        "\n",
        "taxi_id = 0\n",
        "for color in colors:\n",
        "    for t in types:\n",
        "        candidate = {}\n",
        "        candidate['id'] = taxi_id\n",
        "        candidate['name'] = color + ' ' + t\n",
        "        taxi_id += 1\n",
        "        taxi_final.append(candidate)\n",
        "json.dump(taxi_final, new_db, indent = 4)\n",
        "new_db.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7FP9g-9pm_7"
      },
      "source": [
        "##BabyTrie modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrSoljzGztX9",
        "outputId": "de22cdf4-101d-4058-be53-a3dcf39e0ebe"
      },
      "source": [
        "from simple_tokenize import Clean_Text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "DeZaNexTpi6j",
        "outputId": "a4eba0d3-d389-4ef0-a8c6-ae40d9a7d16a"
      },
      "source": [
        "#@title BABYTRIE\n",
        "#%%writefile BabyTrie_MultiWOZ.py\n",
        "import re\n",
        "import json\n",
        "from fastDamerauLevenshtein import damerauLevenshtein as dl\n",
        "from g2p_en import G2p\n",
        "from simple_tokenize import Clean_Text\n",
        "from simple_tokenize import Word_Tokenize\n",
        "\n",
        "def getphoneme(g2p,text):\n",
        "    ptext=g2p(text)\n",
        "    string='-'.join(ptext)\n",
        "    res=string.replace('- -','_')\n",
        "    return res\n",
        "\n",
        "class BabyTrie:#baby version of Trie\n",
        "    class TrieNode:#Node within a Trie\n",
        "        def __init__(self,entity=None):\n",
        "            self.children={}#dictionary of TrieNodes\n",
        "            self.markers=[]#list of markers/categories\n",
        "            self.end=False#the Node is leaf, or end of word\n",
        "            self.entity=None#in case needed\n",
        "            self.id=None\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.root=self.new_node()\n",
        "\n",
        "    def new_node(self,word=None):\n",
        "        #Creates TrieNode object with a given word\n",
        "        return self.TrieNode(word)\n",
        "\n",
        "    def restricted_insert(self,lst,cat,number):\n",
        "        if len(lst) == 0:\n",
        "            print(\"ERROR: empty NE in knowledge file\")\n",
        "            return\n",
        "        if len(lst) == 1 and lst[0]==cat:\n",
        "            print(\"only one elem in lst\")\n",
        "            return\n",
        "        \n",
        "        if lst[0]=='the':#the is ambiguous\n",
        "            lst.pop(0)\n",
        "\n",
        "        ptr=self.root\n",
        "        for i in range(0,len(lst)):#for every elem but not the last (since it is the category)\n",
        "            if lst[i] not in ptr.children.keys():#if already a key, skip; else add new\n",
        "                new=self.new_node(lst[i])\n",
        "                ptr.children[lst[i]]=new\n",
        "            ptr=ptr.children[lst[i]]\n",
        "        #if (len(lst)==2 and lst[-2] in ['good','ask','page']):#we dont want to treat good or ask as an entity (see test.txt)\n",
        "            #ptr.end=False\n",
        "        if (len(lst)==3 and lst[-3]=='cable' and lst[-2]=='car'):\n",
        "            ptr.end=False\n",
        "        else:\n",
        "            ptr.end=True#is end of word\n",
        "            ptr.id=number\n",
        "        if cat not in ptr.markers:#Only adds new categories to the list of markers \n",
        "            ptr.markers.append(cat)\n",
        "    \n",
        "    def insert(self,lst,cat,number):\n",
        "        '''\n",
        "        Given a list of words that make up a Named Entity and its category,\n",
        "        inserts the Entity into the Trie.\n",
        "\n",
        "        Note: the structure resembles a tree, i.e. words as nodes, and at the end\n",
        "        of the inserted word, the node is made a leaf node (end=True), and a Marker\n",
        "        is added to the node's list of markers.\n",
        "        -------------------------------------------------------------------\n",
        "        Example:\n",
        "        bt=BabyTrie()\n",
        "        possible_NE=['Jade','Garden']#let's say that this is a restaurant and id#1\n",
        "        bt.insert(possible_NE,'restaurant',1)\n",
        "        '''\n",
        "        if len(lst) == 0:\n",
        "            print(\"ERROR: empty NE in knowledge file\")\n",
        "            return\n",
        "        if len(lst) == 1 and lst[0]==cat:\n",
        "            print(\"only one elem in lst\")\n",
        "            return\n",
        "        \n",
        "        if lst[0]=='the':#the is ambiguous\n",
        "            lst.pop(0)\n",
        "        \n",
        "        \n",
        "        \n",
        "        if lst[-1]!=cat: #['a','and','b'] + ['restaurant']\n",
        "            lst.append(cat)\n",
        "        '''\n",
        "        for the sake of more possible matches:\n",
        "        If inserted \"Jade Garden\" and it's a restaurant,\n",
        "        \"Jade Garden\" will be recognized as an entity;\n",
        "        and \"Jade Garden Restaurant\" will also be one.\n",
        "        '''\n",
        "        ptr=self.root\n",
        "        for i in range(0,len(lst)-1):#for every elem but not the last (since it is the category)\n",
        "            if lst[i] not in ptr.children.keys():#if already a key, skip; else add new\n",
        "                new=self.new_node(lst[i])\n",
        "                ptr.children[lst[i]]=new\n",
        "            ptr=ptr.children[lst[i]]\n",
        "        if (len(lst)==2 and lst[-2] in ['good','ask','page']):#we dont want to treat good or ask as an entity (see test.txt)\n",
        "            ptr.end=False\n",
        "        elif (len(lst)==3 and lst[-3]=='cable' and lst[-2]=='car'):\n",
        "            ptr.end=False\n",
        "        else:\n",
        "            ptr.end=True#is end of word\n",
        "            ptr.id=number\n",
        "        if cat not in ptr.markers:#Only adds new categories to the list of markers \n",
        "            ptr.markers.append(cat)\n",
        "        if lst[-1] not in ptr.children.keys():#avoid possible conflict with same name\n",
        "            new=self.new_node(lst[-1])\n",
        "            ptr.children[lst[-1]]=new#adds the category as one of the leaf node\n",
        "        ptr=ptr.children[lst[-1]]\n",
        "        ptr.end=True\n",
        "        ptr.id=number\n",
        "        if cat not in ptr.markers:#only adds category if not in list of markers\n",
        "            ptr.markers.append(cat)\n",
        "    \n",
        "    def isinTrie(self,sen):\n",
        "        stutter_match=False\n",
        "        dl_mistakes=['train','want','american','lane','fees','fee','marina','wanna','canna','dinna','finna']\n",
        "        #outfile for tracking log process\n",
        "        dic={}#the uncleaned version of returned dictionary\n",
        "        flag=False#=True when a phrase in the sentence does not match the trie anymore\n",
        "        lst=[]#the list that will ultimately be returned\n",
        "\n",
        "        new_sen=sen.lower().strip()#cleans up sentence\n",
        "\n",
        "        lst=Word_Tokenize(Clean_Text(new_sen))\n",
        "        \n",
        "        last=len(lst)-1#keep track of last to ensure index does not go over limit\n",
        "        #the returned list is completely formed at this point, we use it to generate dictionary\n",
        "        rstart=0#starting position of NE\n",
        "        rend=0#ending position of NE\n",
        "\n",
        "        for i in range(len(lst)):\n",
        "            proceed=False\n",
        "            ptr=self.root\n",
        "            if lst[i] in ptr.children:\n",
        "                proceed=True\n",
        "                rstart=i\n",
        "                ptr=ptr.children[lst[i]]\n",
        "            elif lst[i] not in ptr.children and lst[i] not in dl_mistakes:\n",
        "                for word in ptr.children:\n",
        "                    if len(lst[i])<=4:\n",
        "                        dl_score=dl(lst[i],word,similarity=False,deleteWeight=2,insertWeight=2,replaceWeight=2)\n",
        "                        bm=0\n",
        "                    else:\n",
        "                        dl_score=dl(lst[i],word,similarity=False,deleteWeight=1,insertWeight=2,replaceWeight=2)\n",
        "                        bm=1\n",
        "                    if dl_score<=bm:\n",
        "                        proceed=True\n",
        "                        rstart=i\n",
        "                        ptr=ptr.children[word]\n",
        "                        break\n",
        "            if proceed:\n",
        "                stutter_utt=0\n",
        "                for j in range(i+1,len(lst)):\n",
        "                    if lst[j] in ptr.children:\n",
        "                        ptr=ptr.children[lst[j]]\n",
        "                        rend=j\n",
        "                        stutter_match=False\n",
        "                    elif lst[j] not in ptr.children:\n",
        "                        proceed=False\n",
        "                        for word in ptr.children:\n",
        "                            dl_score=dl(lst[j],word,similarity=False,deleteWeight=2,insertWeight=2,replaceWeight=2)\n",
        "                            bm=1\n",
        "                            if dl_score<=bm and lst[j].isalnum():\n",
        "                                ptr=ptr.children[word]\n",
        "                                rend=j\n",
        "                                proceed=True\n",
        "                                stutter_match=False\n",
        "                                break\n",
        "                        if (len(lst[j])<=3) and (lst[j].isalnum()) and (stutter_utt<1):#allow one mismatch (except for punc), could be stuttering\n",
        "                            stutter_utt+=1\n",
        "                            proceed=True\n",
        "                            rend=j-1\n",
        "                            stutter_match=True\n",
        "                            #here rend not counted since if next is not matched, it will not be effected if the prev word is actually end of word\n",
        "                        elif stutter_match:\n",
        "                            rend=j-2\n",
        "                            break\n",
        "                        #print('for '+lst[i]+' stopped at '+lst[j])#degbug use\n",
        "                        if not proceed:\n",
        "                            rend=j-1\n",
        "                            break\n",
        "            if ptr.end:\n",
        "                for i in range(len(ptr.markers)):\n",
        "                    if (ptr.markers[i] in dic):#if already a key, add to value\n",
        "                        if rstart>rend:\n",
        "                            rend=rstart\n",
        "                        dic[ptr.markers[i]].append(((rstart,rend),ptr.id))\n",
        "                    else:#if not, make new entry\n",
        "                        if rstart>rend:\n",
        "                            rend=rstart\n",
        "                        dic[ptr.markers[i]]=[((rstart,rend),ptr.id)]\n",
        "                rstart=0\n",
        "                rend=0\n",
        "            else:\n",
        "                continue\n",
        "        #cleaning up dictionary\n",
        "        rdic={}#clean version of the dictionary that is ultimately returned\n",
        "        for cat in dic.keys():\n",
        "            rdic[cat]=[]\n",
        "            rlst=sorted(dic[cat],key=lambda item:item[0][1]-item[0][0],reverse=True)\n",
        "            #rdic[cat].append(rlst[0])\n",
        "            for i in range(0,len(rlst)):\n",
        "                rflag=True\n",
        "                for rcat in rdic.keys():#make sure no overlap: ex. 'some hotel diner' > 'some hotel'\n",
        "                    for elem in rdic[rcat]:\n",
        "                        if (rlst[i][0][0]<elem[0][0] and rlst[i][0][1]<elem[0][0]):#ex. if we have (3,5) we can take (1,2)\n",
        "                            continue\n",
        "                        elif (rlst[i][0][0]>elem[0][1] and rlst[i][0][1]>elem[0][1]):#ex. if we have (3,5) we can take (6,7)\n",
        "                            continue\n",
        "                        else:# any equals, [0] or [1] will not work: ex. we have (3,5). cannot have (3,4) or (4,5).\n",
        "                            if rlst[i][0][1]-rlst[i][0][0]==elem[0][1]-elem[0][0] and rlst[i][0][1]>elem[0][1]:\n",
        "                                rdic[rcat].remove(elem)\n",
        "                                continue\n",
        "                            else:\n",
        "                                rflag=False\n",
        "                if rflag:\n",
        "                    rdic[cat].append(rlst[i])\n",
        "        del_lst=[]#cheap(costly) way to deal with empty dic entry\n",
        "        for cat in rdic:\n",
        "            if len(rdic[cat])==0:\n",
        "                del_lst.append(cat)\n",
        "        for elem in del_lst:\n",
        "            del rdic[elem]\n",
        "        return (lst,rdic)\n",
        "        \n",
        "    \n",
        "\n",
        "    def initialize_multiwoz(self):\n",
        "        dic_lst = ['/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/attraction_db.json',\n",
        "                   '/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/restaurant_db.json',\n",
        "                   '/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/hotel_db.json',\n",
        "                   '/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/police_db.json',\n",
        "                   '/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/hospital_db_clean.json',\n",
        "                   '/content/drive/MyDrive/Research/ner/Multiwoz/multiwoz/db/taxi_db_clean.json',        \n",
        "        ]\n",
        "        cat_lst = ['attraction',\n",
        "                   'restaurant',\n",
        "                   'hotel',\n",
        "                   'police',\n",
        "                   'hospital',\n",
        "                   'taxi',\n",
        "        ]\n",
        "\n",
        "        for f in range(len(dic_lst)):\n",
        "            print('Starting ', dic_lst[f].split('/')[-1])\n",
        "            db = open(dic_lst[f],'r')\n",
        "            dic = json.load(db)\n",
        "            db.close()\n",
        "            cat = cat_lst[f]\n",
        "            for record in dic:            \n",
        "                if ('name' in record and record['name'] is not None) or ('department' in record):\n",
        "                    if cat == 'hospital':\n",
        "                        new_name=record['department'].lower().strip()\n",
        "                    else:\n",
        "                        if record['name'] == 'the place':#edge case\n",
        "                            continue\n",
        "                        new_name=record['name'].lower().strip()#cleans up name\n",
        "                    elem = record['id']\n",
        "                    if new_name == cat:\n",
        "                        print('exceptional entity---------------------')\n",
        "                        continue\n",
        "                    lst=[]#final list that is used to insert into bt\n",
        "                    lst=Word_Tokenize(Clean_Text(new_name))\n",
        "                    #p=re.compile(r\"^(\\w+)(\\'\\w+)$\")\n",
        "\n",
        "                    \n",
        "                    self.restricted_insert(lst,cat,elem)\n",
        "                    if cat == 'police':\n",
        "                        self.restricted_insert(lst[:-1], cat, elem)\n",
        "                    #self.restricted_insert(no_punc_lst,cat,elem)#Tokenize will not split '-' such that 'alpha-milton guest house' will result in inserting 'guest house'\n",
        "                    \n",
        "                    if 'and' in lst and 'bar' in lst:#de luca cucina and bar\n",
        "                        partial=lst[0:lst.index('and')]\n",
        "                        self.insert(partial,cat,elem)\n",
        "                        \n",
        "                    for i in range(len(lst)):#& <=> and\n",
        "                        if lst[i]=='&':\n",
        "                            lst[i]='and'\n",
        "                            self.restricted_insert(lst,cat,elem)\n",
        "                        elif lst[i]=='and':\n",
        "                            lst[i]='&'\n",
        "                            self.restricted_insert(lst,cat,elem)\n",
        "    \n",
        "                    if ',' in lst:#hotel vitale, ...\n",
        "                        partial=lst[0:lst.index(',')]\n",
        "                        self.restricted_insert(partial,cat,elem)\n",
        "                        for i in range(len(partial)):#& <=> and\n",
        "                            if partial[i]=='&':\n",
        "                                partial[i]='and'\n",
        "                                self.restricted_insert(partial,cat,elem)\n",
        "                            elif partial[i]=='and':\n",
        "                                partial[i]='&'\n",
        "                                self.restricted_insert(partial,cat,elem)\n",
        "\n",
        "    def gettemplate_wmap(self, g2p, title):\n",
        "        title = re.sub(r'\\u2019', \"'\", title)\n",
        "        title = Clean_Text(title).replace('\\n',' ')\n",
        "        nltk=' '.join(Word_Tokenize(title))\n",
        "    \n",
        "        word_lst,ind_dic=self.isinTrie(title)\n",
        "        nlst=Word_Tokenize(Clean_Text(title))\n",
        "        new_sen=title.strip()#cleans up sentence\n",
        "        \n",
        "        en_dic={}\n",
        "        title_phoneme=title\n",
        "        for key in ind_dic:\n",
        "            ind_lst=ind_dic[key]\n",
        "            for pair in ind_lst:\n",
        "                temp_str=''\n",
        "                for index in range(pair[0][0],pair[0][1]+1):\n",
        "                    if not nlst[index][0].isalnum():\n",
        "                        temp_str=temp_str[:-1]\n",
        "                    if nlst[index]=='-':\n",
        "                        temp_str+=nlst[index]\n",
        "                    else:\n",
        "                        temp_str+=nlst[index]+' '\n",
        "                temp_str=temp_str[:-1]#getting rid of last space\n",
        "                #temp_str_converted=convert_text2num(temp_str)#name entity to be in dig form\n",
        "                phoneme_temp_str=getphoneme(g2p,temp_str)\n",
        "                en_dic['<'+key+'-'+str(pair[1])+'>']=temp_str\n",
        "                title_phoneme=title_phoneme.replace(temp_str,'<'+phoneme_temp_str+'>')\n",
        "                title=title.replace(temp_str,'<'+key+'-'+str(pair[1])+'>')\n",
        "        return [title,en_dic,title_phoneme,nltk]                    \n",
        "def main():\n",
        "    g2p = G2p()\n",
        "    bt=BabyTrie()\n",
        "    bt.initialize_multiwoz()\n",
        "    test=\"The taxi is all set, look for a grey toyota, they can be reached at 07596698267 if there are any issues. Any other questions?\"\n",
        "    res=bt.isinTrie(test)\n",
        "    #print(res[0],res[1],res[2],sep='\\n')\n",
        "    print(res)\n",
        "    res2 = bt.gettemplate_wmap(g2p,test)\n",
        "    print(res2[0],res2[1],res2[2],res2[3],sep='\\n')\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting BabyTrie_MultiWOZ.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ0qUkmLHUS6",
        "outputId": "5e924584-8cf4-474f-a20b-cfe3483e740f"
      },
      "source": [
        "#Quick Test\n",
        "from BabyTrie_MultiWOZ import BabyTrie\n",
        "\n",
        "g2p = G2p()\n",
        "bt = BabyTrie()\n",
        "bt.insert(['amc','theatre'],'attraction','0')\n",
        "bt.insert(['amc','theatre'],'movie','0')\n",
        "#bt.insert(['amc','theatre'],'attraction','0')\n",
        "res = bt.gettemplate_wmap(g2p, 'amc theatre is great.')\n",
        "print(res[0], res[1], res[2], res[3], sep = '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<attraction-0> is great.\n",
            "{'<attraction-0>': 'amc theatre'}\n",
            "<EY1-EH1-M-S-IY1_TH-IY1-AH0-T-ER0> is great.\n",
            "amc theatre is great .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U54721F5al2f"
      },
      "source": [
        "##Outputs (need change directory accordingly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPFXQVUtbPhe"
      },
      "source": [
        "MultiWOZ2.2 train, test, dev templates\n",
        "Note: only restaurant, hotel, and attraction are 'full-proof'.\n",
        "\n",
        "1. attraction has an entity 'the place', edgecase.\n",
        "2. hospital actual name is not avaliable in db, thus not inserted into trie.\n",
        "3. taxi entities are enumerated from the old json file with 'color + type'.\n",
        "4. bus/train have the same db.json and they are not currently being considered.\n",
        "\n",
        "a. look for more edgecases\n",
        "b. considering bus/train: areas (destination/departure)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGS4S-m6HhTa",
        "outputId": "d92e21ba-6836-40b5-98ac-4f710867a629"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from g2p_en import G2p\n",
        "\n",
        "g2p = G2p()\n",
        "bt=BabyTrie()\n",
        "bt.initialize_multiwoz()\n",
        "\n",
        "\n",
        "directory = '../dev/'#../train/ or ../test/\n",
        "for filename in os.listdir(directory):\n",
        "    print('Now: '+filename)\n",
        "    f = open(directory+filename,'r')\n",
        "    log = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "    for dialogue in log:\n",
        "        for turn in dialogue['turns']:\n",
        "            text = turn['utterance']\n",
        "            res = bt.gettemplate_wmap(g2p, text)\n",
        "            turn['text_tmplate'] = res[0]\n",
        "            turn['id_map'] = res[1]\n",
        "            turn['text_phoneme'] = res[2]\n",
        "            turn['text_nltk'] = res[3]\n",
        "\n",
        "\n",
        "    out = directory+filename.split('.')[0]+'_template.json'\n",
        "    wf = open(out,'w')\n",
        "    json.dump(log, wf, indent = 4)\n",
        "    wf.close()\n",
        "    print('Done.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting  attraction_db.json\n",
            "Starting  restaurant_db.json\n",
            "Starting  hotel_db.json\n",
            "Starting  police_db.json\n",
            "Starting  hospital_db_clean.json\n",
            "Starting  taxi_db_clean.json\n",
            "Now: dialogues_001.json\n",
            "Done.\n",
            "Now: dialogues_002.json\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUx1fk7aYgub",
        "outputId": "2184e83f-6da8-440d-ec24-00c8f4f45a6d"
      },
      "source": [
        "%cd ../dev/#../train/ or ../test/\n",
        "!ls -lrt\n",
        "!zip multiwoz_dev_templates.zip *_template.json#rename if needed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Research/ner/Multiwoz/multiwoz/data/MultiWOZ_2.2/dev\n",
            "total 59521\n",
            "-rw------- 1 root root 10708830 Jun 29 08:20 dialogues_002.json\n",
            "-rw------- 1 root root 11771811 Jun 29 08:20 dialogues_001.json\n",
            "-rw------- 1 root root 20128689 Aug  7 11:36 dialogues_001_template.json\n",
            "-rw------- 1 root root 18339329 Aug  7 11:37 dialogues_002_template.json\n",
            "  adding: dialogues_001_template.json (deflated 96%)\n",
            "  adding: dialogues_002_template.json (deflated 96%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRorsnBla7mF"
      },
      "source": [
        "# Task 2: MultiWOZ data preperation for Intent Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh3cBW_5ty-Z"
      },
      "source": [
        "Abandoned for now since MultiWOZ data has similar utterances as the KB titles.\n",
        "Use data modified from DSTC9 set instead:\n",
        "\n",
        "/scratch/yt2267/public/alexa-with-dstc9-track1-dataset/data_trainval/train/logs.json, labels.json, use target=False and last user turn for non-KB utterances @Seeger Zouâ€…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKGpljFKbDkJ"
      },
      "source": [
        "##Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6JWHBhAbCU_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}